{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7e2a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import shutil\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f9477bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your OpenAI API key:\n",
      "✓ API key set successfully: sk-proj...Tr8A\n"
     ]
    }
   ],
   "source": [
    "# Prompt for OpenAI API key with password masking\n",
    "print(\"Please enter your OpenAI API key:\")\n",
    "openai_api_key = getpass.getpass(\"API Key: \")\n",
    "\n",
    "# Set as environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# Verify it was set (show only first/last few characters for security)\n",
    "if openai_api_key:\n",
    "    masked_key = f\"{openai_api_key[:7]}...{openai_api_key[-4:]}\"\n",
    "    print(f\"✓ API key set successfully: {masked_key}\")\n",
    "else:\n",
    "    print(\"✗ No API key entered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e609e072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_s/4t2p1z2x0yxcv068dtqj31tw0000gq/T/ipykernel_10292/1915034900.py:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "\n",
    "# Initialize vector store\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"HTML_samples_italian\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory='./chroma-db_italian'\n",
    ")\n",
    "\n",
    "# Configure text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=300,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "12e2ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# metadata extraction function\n",
    "\n",
    "\n",
    "# metadata extraction function\n",
    "def extract_metadata(soup, html_path):\n",
    "    # Convert to Path object if it's a string\n",
    "    if isinstance(html_path, str):\n",
    "        html_path = Path(html_path)\n",
    "    \n",
    "    content_main = soup.find(\"main\", class_=\"content\")\n",
    "    if not content_main:\n",
    "        return None\n",
    "\n",
    "    metadata_div = content_main.find(\"div\")\n",
    "    if not metadata_div:\n",
    "        return None\n",
    "\n",
    "    paragraphs = metadata_div.find_all(\"p\")\n",
    "    title = author = None\n",
    "\n",
    "    for p in paragraphs:\n",
    "        text = p.get_text(strip=True)\n",
    "        if text.startswith(\"Title:\"):\n",
    "            title = text.split(\":\", 1)[-1].strip()\n",
    "        elif text.startswith(\"Author:\"):\n",
    "            author = text.split(\":\", 1)[-1].strip()\n",
    "\n",
    "    source_div = metadata_div.find(\"div\")\n",
    "    source_text = source_div.get_text(separator=\"\\n\", strip=True) if source_div else \"\"\n",
    "\n",
    "    return {\n",
    "        \"Filename\": html_path.name,\n",
    "        \"Title\": title or \"\",\n",
    "        \"Author\": author or \"\",\n",
    "        \"Source\": source_text,\n",
    "        \"Citation\": f\"https://tmiweb.science.uu.nl/text/reading-edition/{html_path.name}\"\n",
    "    }\n",
    "\n",
    "# # main page content extraction\n",
    "# main page content extraction\n",
    "def extract_pages_and_text(html_content, html_path):\n",
    "    \"\"\"\n",
    "    Extract page numbers and their associated text from TEI HTML.\n",
    "    \n",
    "    Page numbers are in <span class=\"tei pb\"><span class=\"tei pbSpan\">page X</span></span>\n",
    "    Page content follows each page break as sibling elements until the next page break.\n",
    "    \n",
    "    Args:\n",
    "        html_content: Either a string containing TEI HTML or a BeautifulSoup object\n",
    "        html_path: Path to the HTML file    \n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with 'PageNumber', 'PageText', and metadata\n",
    "    \"\"\"\n",
    "    # Check if html_content is already a BeautifulSoup object\n",
    "    if isinstance(html_content, BeautifulSoup):\n",
    "        soup = html_content\n",
    "    else:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    metadata = extract_metadata(soup, html_path)\n",
    "    \n",
    "    # Find all page break tags - these are <span class=\"tei pb\"> elements\n",
    "    page_breaks = soup.find_all('span', class_='tei pb')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, pb in enumerate(page_breaks):\n",
    "        # Extract page number from the nested pbSpan\n",
    "        # Structure: <span class=\"tei pb\"><span class=\"tei pbSpan\">page X</span></span>\n",
    "        page_span = pb.find('span', class_='tei pbSpan')\n",
    "        page_number = page_span.get_text(strip=True) if page_span else f\"page {i+1}\"\n",
    "        \n",
    "        # Collect ALL text content that follows this page break until the next one\n",
    "        # Strategy: Get all text nodes between the two page breaks in document order\n",
    "        \n",
    "        # Get the next page break to know where to stop\n",
    "        next_pb = page_breaks[i + 1] if i + 1 < len(page_breaks) else None\n",
    "        \n",
    "        # Collect all NavigableStrings (text nodes) between page breaks\n",
    "        page_text_parts = []\n",
    "        \n",
    "        # Start after current page break\n",
    "        for element in pb.next_elements:\n",
    "            # Stop if we hit the next page break\n",
    "            if next_pb and element == next_pb:\n",
    "                break\n",
    "            \n",
    "            # Only collect NavigableString objects (actual text nodes)\n",
    "            # This avoids double-counting when we have nested tags\n",
    "            from bs4 import NavigableString\n",
    "            if isinstance(element, NavigableString):\n",
    "                text = str(element).strip()\n",
    "                if text:\n",
    "                    page_text_parts.append(text)\n",
    "        \n",
    "        page_text = ' '.join(page_text_parts)\n",
    "        \n",
    "        result_entry = {\n",
    "            'PageNumber': page_number,\n",
    "            'PageText': page_text\n",
    "        }\n",
    "        \n",
    "        # Attach metadata if available\n",
    "        if metadata:\n",
    "            result_entry.update(metadata)\n",
    "        \n",
    "        results.append(result_entry)\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a776ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this on all files in the source directory:  this is just to get the text, not the vector db\n",
    "tei_dir = Path(\"tei_source\")\n",
    "for html_path in tei_dir.glob(\"*.html\"):\n",
    "    with html_path.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "        html_content = handle.read()    \n",
    "\n",
    "        results = extract_pages_and_text(html_content, html_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d7fd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PageNumber': 'page i',\n",
       " 'PageText': \"page i DIALOGO DEL R. M. DON PIETRO PONTIO PARMIGIANO, Oue si tratta della Theorica, e Prattica di Musica. \\n                        Et anco si mostra la diuersità de' Contraponti, & Canoni. [Figure] In Parma , appresso Erasmo Viothi . 1595. Con licenza de' Superiori.\",\n",
       " 'Filename': 'pondia.html',\n",
       " 'Title': 'Dialogo di musica',\n",
       " 'Author': 'Pietro Pontio',\n",
       " 'Source': 'Copyright © 2001, Utrecht University, Netherlands',\n",
       " 'Citation': 'https://tmiweb.science.uu.nl/text/reading-edition/pondia.html'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to check one result\n",
    "\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85d2f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_files(html_dir='tei_source'):\n",
    "    \"\"\"Process all HTML files in the specified directory.\"\"\"\n",
    "    html_files = glob.glob(os.path.join(html_dir, '*.html'))\n",
    "    \n",
    "    if not html_files:\n",
    "        print(f\"No HTML files found in {html_dir}\")\n",
    "        return\n",
    "    \n",
    "    total_chunks = 0\n",
    "    total_pages = 0\n",
    "    \n",
    "    for filepath in html_files:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                html_content = f.read()\n",
    "            \n",
    "            # Parse HTML\n",
    "            pages = extract_pages_and_text(html_content, Path(filepath))\n",
    "            \n",
    "            # # Extract metadata from the record section\n",
    "            # doc_metadata = extract_metadata(soup)\n",
    "            \n",
    "            # # Get document-level title\n",
    "            # doc_title = get_document_title(soup, doc_metadata)\n",
    "            \n",
    "            # # Extract pages\n",
    "            # pages = extract_pages(soup)\n",
    "            \n",
    "            if not pages:\n",
    "                print(f\"Warning: No pages found in {filepath}\")\n",
    "                continue\n",
    "            \n",
    "            file_chunks = 0\n",
    "            \n",
    "            # Process each page separately\n",
    "            for page in pages:\n",
    "                # Create metadata for this page, combining document and page metadata\n",
    "                page_metadata = {\n",
    "                    \"title\": page['Title'],\n",
    "                    \"author\": page['Author'],\n",
    "                    \"source\": page['Source'],\n",
    "                    \"citation\": page['Citation'],\n",
    "                    \"page_number\": page['PageNumber']\n",
    "                }\n",
    "                \n",
    "                # Split page text into chunks\n",
    "                chunks = text_splitter.create_documents(\n",
    "                    texts=[page['PageText']],\n",
    "                    metadatas=[page_metadata]\n",
    "                )\n",
    "                \n",
    "                # Add chunks to vector store\n",
    "                vector_store.add_documents(documents=chunks)\n",
    "                \n",
    "                file_chunks += len(chunks)\n",
    "                total_chunks += len(chunks)\n",
    "            \n",
    "            total_pages += len(pages)\n",
    "            print(f'✓ {os.path.basename(filepath)}')\n",
    "            print(f'  Title: {page[\"Title\"][:80]}...' if len(page[\"Title\"]) > 80 else f'  Title: {page[\"Title\"]}')\n",
    "            print(f'  Author: {page[\"Author\"]}')\n",
    "            print(f'  Citation: {page[\"Citation\"]}')\n",
    "            print(f'  Pages: {len(pages)} | Chunks: {file_chunks}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing {filepath}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'ChromaDB Processing Complete')\n",
    "    print(f'{\"=\"*50}')\n",
    "    print(f'Files processed: {len(html_files)}')\n",
    "    print(f'Total pages: {total_pages}')\n",
    "    print(f'Total chunks: {total_chunks}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5725cafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ artart.html\n",
      "  Title: L'Artusi\n",
      "  Author: Giovanni Maria Artusi\n",
      "  Citation: https://tmiweb.science.uu.nl/text/reading-edition/artart.html\n",
      "  Pages: 155 | Chunks: 314\n",
      "✓ zarins58.html\n",
      "  Title: Le istitutioni harmoniche\n",
      "  Author: Gioseffo Zarlino\n",
      "  Citation: https://tmiweb.science.uu.nl/text/reading-edition/zarins58.html\n",
      "  Pages: 353 | Chunks: 1027\n",
      "✓ tigcom.html\n",
      "  Title: Il compendio della musica\n",
      "  Author: Orazio Tigrini\n",
      "  Citation: https://tmiweb.science.uu.nl/text/reading-edition/tigcom.html\n",
      "  Pages: 146 | Chunks: 254\n",
      "✓ vicant.html\n",
      "  Title: L'antica musica ridotta alla moderna prattica\n",
      "  Author: Nicola Vicentino\n",
      "  Citation: https://tmiweb.science.uu.nl/text/reading-edition/vicant.html\n",
      "  Pages: 276 | Chunks: 574\n",
      "✓ pondia.html\n",
      "  Title: Dialogo di musica\n",
      "  Author: Pietro Pontio\n",
      "  Citation: https://tmiweb.science.uu.nl/text/reading-edition/pondia.html\n",
      "  Pages: 160 | Chunks: 224\n",
      "\n",
      "==================================================\n",
      "ChromaDB Processing Complete\n",
      "==================================================\n",
      "Files processed: 5\n",
      "Total pages: 1090\n",
      "Total chunks: 2393\n"
     ]
    }
   ],
   "source": [
    "process_html_files(html_dir='tei_source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da0cf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total pages: 155\n",
      "  Average PageText length: 3025.00 characters\n",
      "  Min length: 24 characters\n",
      "  Max length: 4215 characters\n"
     ]
    }
   ],
   "source": [
    "# for checking page and character counts\n",
    "\n",
    "\n",
    "pages = results\n",
    "        # Calculate lengths of all PageText entries\n",
    "page_lengths = [len(page['PageText']) for page in pages]\n",
    "avg_length = sum(page_lengths) / len(page_lengths)\n",
    "    \n",
    "# print(f\"\\n{filename}:\")\n",
    "print(f\"  Total pages: {len(pages)}\")\n",
    "print(f\"  Average PageText length: {avg_length:.2f} characters\")\n",
    "print(f\"  Min length: {min(page_lengths)} characters\")\n",
    "print(f\"  Max length: {max(page_lengths)} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5614ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another check\n",
    "\n",
    "for page in pages:\n",
    "    if len(page['PageText']) > 3000:\n",
    "        print(f\"long page found: {page['PageNumber']} with length {len(page['PageText'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
