{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67350748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment and api key\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "import shutil\n",
    "import getpass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d34cd2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your OpenAI API key:\n",
      "✓ API key set successfully: sk-proj...fq4A\n"
     ]
    }
   ],
   "source": [
    "# Prompt for OpenAI API key with password masking\n",
    "print(\"Please enter your OpenAI API key:\")\n",
    "openai_api_key = getpass.getpass(\"API Key: \")\n",
    "\n",
    "# Set as environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# Verify it was set (show only first/last few characters for security)\n",
    "if openai_api_key:\n",
    "    masked_key = f\"{openai_api_key[:7]}...{openai_api_key[-4:]}\"\n",
    "    print(f\"✓ API key set successfully: {masked_key}\")\n",
    "else:\n",
    "    print(\"✗ No API key entered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92f68af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ morley_html_source.html\n",
      "  Title: A plaine and easie introduction to practicall musicke set downe in forme of a di...\n",
      "  Author: Morley, Thomas, 1557-1603?\n",
      "  Pages: 230 | Chunks: 364\n",
      "✓ ornithiparcus_html_source.html\n",
      "  Title: Andreas Ornithoparcus his Micrologus, or Introduction: containing the art of sin...\n",
      "  Author: Ornithoparchus, Andreas, 16th cent.\n",
      "  Pages: 102 | Chunks: 146\n",
      "\n",
      "==================================================\n",
      "ChromaDB Processing Complete\n",
      "==================================================\n",
      "Files processed: 2\n",
      "Total pages: 332\n",
      "Total chunks: 510\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "# Initialize vector store\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"HTML_samples\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory='./chroma-db'\n",
    ")\n",
    "\n",
    "# Configure text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=300,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "def extract_metadata(soup):\n",
    "    \"\"\"Extract metadata from the HTML record section.\"\"\"\n",
    "    metadata = {\n",
    "        'title': '',\n",
    "        'author': '',\n",
    "        'pub_info': '',\n",
    "        'citation': ''\n",
    "    }\n",
    "    \n",
    "    # Find the record dl element\n",
    "    record_dl = soup.find('dl', class_='record')\n",
    "    \n",
    "    if not record_dl:\n",
    "        return metadata\n",
    "    \n",
    "    # Extract title\n",
    "    title_div = record_dl.find('div', {'data-key': 'title'})\n",
    "    if title_div:\n",
    "        title_dd = title_div.find('dd')\n",
    "        if title_dd:\n",
    "            metadata['title'] = title_dd.get_text(strip=True)\n",
    "    \n",
    "    # Extract author\n",
    "    author_div = record_dl.find('div', {'data-key': 'author'})\n",
    "    if author_div:\n",
    "        author_dd = author_div.find('dd')\n",
    "        if author_dd:\n",
    "            metadata['author'] = author_dd.get_text(strip=True)\n",
    "    \n",
    "    # Extract publication info (combine all dd elements)\n",
    "    pubinfo_div = record_dl.find('div', {'data-key': 'pubinfo'})\n",
    "    if pubinfo_div:\n",
    "        pubinfo_dds = pubinfo_div.find_all('dd')\n",
    "        pub_parts = [dd.get_text(strip=True) for dd in pubinfo_dds]\n",
    "        metadata['pub_info'] = ' '.join(pub_parts)\n",
    "    \n",
    "    # Extract citation\n",
    "    citation_div = record_dl.find('div', {'data-key': 'citation'})\n",
    "    if citation_div:\n",
    "        citation_dd = citation_div.find('dd')\n",
    "        if citation_dd:\n",
    "            # Get text from span, preserving the citation format\n",
    "            citation_span = citation_dd.find('span')\n",
    "            if citation_span:\n",
    "                # Get the text content, replacing <em> tags\n",
    "                citation_text = citation_span.decode_contents()\n",
    "                # Parse again to get clean text\n",
    "                citation_soup = BeautifulSoup(citation_text, 'html.parser')\n",
    "                metadata['citation'] = citation_soup.get_text(strip=True)\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def get_document_title(soup, metadata):\n",
    "    \"\"\"Extract the overall document title from HTML or metadata.\"\"\"\n",
    "    # Use metadata title if available\n",
    "    if metadata.get('title'):\n",
    "        return metadata['title']\n",
    "    \n",
    "    # Fallback to HTML title tag\n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag and title_tag.text.strip():\n",
    "        return title_tag.text.strip()\n",
    "    \n",
    "    # Try h1 or h2 for document title\n",
    "    h1 = soup.find('h1')\n",
    "    if h1:\n",
    "        return h1.text.strip()\n",
    "    \n",
    "    h2 = soup.find('h2')\n",
    "    if h2:\n",
    "        return h2.text.strip()\n",
    "    \n",
    "    return \"Untitled Document\"\n",
    "\n",
    "def extract_pages(soup):\n",
    "    \"\"\"Extract individual pages with their metadata from the HTML.\"\"\"\n",
    "    pages = []\n",
    "    \n",
    "    # Find all article elements that represent pages\n",
    "    articles = soup.find_all('article', class_='fullview-page')\n",
    "    \n",
    "    for article in articles:\n",
    "        # Extract page metadata from the h3 heading\n",
    "        page_heading = article.find('h3', class_='js-toc-ignore')\n",
    "        \n",
    "        if page_heading:\n",
    "            page_num = page_heading.get('data-p-num', 'Unknown')\n",
    "            page_label = page_heading.get('data-heading-label', f'Page {page_num}')\n",
    "            base = page_heading.get('data-base', '')\n",
    "        else:\n",
    "            page_num = 'Unknown'\n",
    "            page_label = 'Unknown Page'\n",
    "            base = ''\n",
    "        \n",
    "        # Extract text content from this page, removing script/style\n",
    "        for script in article([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get clean text from the page\n",
    "        text = article.get_text(separator=' ', strip=True)\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        if text.strip():\n",
    "            pages.append({\n",
    "                'text': text,\n",
    "                'page_number': page_num,\n",
    "                'page_label': page_label,\n",
    "                'base': base\n",
    "            })\n",
    "    \n",
    "    return pages\n",
    "\n",
    "def process_html_files(html_dir='html_source'):\n",
    "    \"\"\"Process all HTML files in the specified directory.\"\"\"\n",
    "    html_files = glob.glob(os.path.join(html_dir, '*.html'))\n",
    "    \n",
    "    if not html_files:\n",
    "        print(f\"No HTML files found in {html_dir}\")\n",
    "        return\n",
    "    \n",
    "    total_chunks = 0\n",
    "    total_pages = 0\n",
    "    \n",
    "    for filename in html_files:\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                html_content = f.read()\n",
    "            \n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "            # Extract metadata from the record section\n",
    "            doc_metadata = extract_metadata(soup)\n",
    "            \n",
    "            # Get document-level title\n",
    "            doc_title = get_document_title(soup, doc_metadata)\n",
    "            \n",
    "            # Extract pages\n",
    "            pages = extract_pages(soup)\n",
    "            \n",
    "            if not pages:\n",
    "                print(f\"Warning: No pages found in {filename}\")\n",
    "                continue\n",
    "            \n",
    "            file_chunks = 0\n",
    "            \n",
    "            # Process each page separately\n",
    "            for page in pages:\n",
    "                # Create metadata for this page, combining document and page metadata\n",
    "                page_metadata = {\n",
    "                    \"document_title\": doc_title,\n",
    "                    \"title\": doc_metadata['title'],\n",
    "                    \"author\": doc_metadata['author'],\n",
    "                    \"pub_info\": doc_metadata['pub_info'],\n",
    "                    \"citation\": doc_metadata['citation'],\n",
    "                    \"page_number\": page['page_number'],\n",
    "                    \"page_label\": page['page_label'],\n",
    "                    \"base\": page['base'],\n",
    "                    \"source_file\": os.path.basename(filename)\n",
    "                }\n",
    "                \n",
    "                # Split page text into chunks\n",
    "                chunks = text_splitter.create_documents(\n",
    "                    texts=[page['text']],\n",
    "                    metadatas=[page_metadata]\n",
    "                )\n",
    "                \n",
    "                # Add chunks to vector store\n",
    "                vector_store.add_documents(documents=chunks)\n",
    "                \n",
    "                file_chunks += len(chunks)\n",
    "                total_chunks += len(chunks)\n",
    "            \n",
    "            total_pages += len(pages)\n",
    "            print(f'✓ {os.path.basename(filename)}')\n",
    "            print(f'  Title: {doc_metadata[\"title\"][:80]}...' if len(doc_metadata[\"title\"]) > 80 else f'  Title: {doc_metadata[\"title\"]}')\n",
    "            print(f'  Author: {doc_metadata[\"author\"]}')\n",
    "            print(f'  Pages: {len(pages)} | Chunks: {file_chunks}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing {filename}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'ChromaDB Processing Complete')\n",
    "    print(f'{\"=\"*50}')\n",
    "    print(f'Files processed: {len(html_files)}')\n",
    "    print(f'Total pages: {total_pages}')\n",
    "    print(f'Total chunks: {total_chunks}')\n",
    "\n",
    "def query_example(query_text, n_results=5):\n",
    "    \"\"\"Example function to query the vector store.\"\"\"\n",
    "    results = vector_store.similarity_search(query_text, k=n_results)\n",
    "    \n",
    "    print(f\"\\nQuery: '{query_text}'\")\n",
    "    print(f\"Found {len(results)} results:\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"Result {i}:\")\n",
    "        print(f\"  Title: {doc.metadata.get('title', 'N/A')[:80]}...\")\n",
    "        print(f\"  Author: {doc.metadata.get('author', 'N/A')}\")\n",
    "        print(f\"  Page: {doc.metadata.get('page_label', 'N/A')}\")\n",
    "        print(f\"  Citation: {doc.metadata.get('citation', 'N/A')[:100]}...\")\n",
    "        print(f\"  Source: {doc.metadata.get('source_file', 'N/A')}\")\n",
    "        print(f\"  Content preview: {doc.page_content[:200]}...\")\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure the HTML directory exists\n",
    "    html_dir = 'html_source'\n",
    "    if not os.path.exists(html_dir):\n",
    "        print(f\"Error: Directory '{html_dir}' not found\")\n",
    "        print(f\"Please create the '{html_dir}' directory and add your HTML files\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Process files\n",
    "    process_html_files(html_dir)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
